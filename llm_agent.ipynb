{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manideepvangari/IAT339-P1-Git-Test/blob/main/llm_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV96XeLPUtmn"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IAT-ComputationalCreativity-Spring2025/Week4-Cognitive-Agents/blob/main/llm_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AepsIILCUtms"
      },
      "source": [
        "# Building a Memory-Aware Cognitive Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWU0RnoEUtmt"
      },
      "source": [
        "## Pre-downloading the Phi-2 Model\n",
        "\n",
        "Before we begin the lab exercises, we'll download and cache the Phi-2 model locally. This is important because:\n",
        "- The model is large (approximately 14GB)\n",
        "- Downloading at the start prevents delays during exercises\n",
        "- Local caching means we only need to download once\n",
        "\n",
        "⚠️ **Requirements**:\n",
        "- At least 16GB of free disk space\n",
        "- Stable internet connection\n",
        "- Hugging Face account and token\n",
        "- Accepted model license\n",
        "\n",
        "Please make sure you have:\n",
        "1. Created a Hugging Face account at https://huggingface.co/join\n",
        "2. Generated your access token at https://huggingface.co/settings/tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mugEBoK_Utmv"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install required packages\n",
        "! pip install torch transformers huggingface_hub accelerate>=0.26.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK9MA5MDUtmx"
      },
      "outputs": [],
      "source": [
        "# Step 2: Import necessary libraries\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from huggingface_hub import login, snapshot_download\n",
        "from pathlib import Path\n",
        "\n",
        "def setup_and_download_model(token=None, model_id='microsoft/phi-2', cache_dir=\"./model_cache\"):\n",
        "    \"\"\"\n",
        "    Set up authentication and pre-download the model files.\n",
        "\n",
        "    Args:\n",
        "        token (str): Hugging Face token\n",
        "        model_id (str): Model identifier on Hugging Face\n",
        "        cache_dir (str): Directory to store the model files\n",
        "    \"\"\"\n",
        "    if token is None:\n",
        "        token = os.getenv('HUGGINGFACE_TOKEN')\n",
        "        if token is None:\n",
        "            raise ValueError(\n",
        "                \"Please provide your Hugging Face token either as an argument or \"\n",
        "                \"set it as HUGGINGFACE_TOKEN environment variable. \"\n",
        "                \"You can get your token from https://huggingface.co/settings/tokens\"\n",
        "            )\n",
        "\n",
        "    # Login to Hugging Face\n",
        "    login(token=token)\n",
        "    print(\"Authenticated with Hugging Face!\")\n",
        "\n",
        "    # Create cache directory if it doesn't exist\n",
        "    cache_dir = Path(cache_dir)\n",
        "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        print(f\"\\nDownloading {model_id} to {cache_dir}...\")\n",
        "        print(\"This may take a while depending on your internet connection...\")\n",
        "\n",
        "        # Download model files\n",
        "        snapshot_download(\n",
        "            repo_id=model_id,\n",
        "            local_dir=cache_dir,\n",
        "            token=token,\n",
        "            ignore_patterns=[\"*.md\", \"*.h5\", \"*.ot\", \"*.pt\"],  # Exclude unnecessary files\n",
        "        )\n",
        "\n",
        "        print(\"\\nVerifying the download by loading model and tokenizer...\")\n",
        "\n",
        "        # Try loading the model to verify the download\n",
        "        tokenizer = AutoTokenizer.from_pretrained(cache_dir)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            cache_dir,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "\n",
        "        print(\"\\nSuccess! Model and tokenizer have been downloaded and verified.\")\n",
        "        print(f\"Model files are cached in: {cache_dir}\")\n",
        "\n",
        "        return tokenizer, model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during download/verification: {str(e)}\")\n",
        "        print(\"\\nPlease ensure:\")\n",
        "        print(\"1. You have accepted the model's license at:\")\n",
        "        print(f\"   https://huggingface.co/{model_id}\")\n",
        "        print(\"2. You have sufficient disk space\")\n",
        "        print(\"3. Your internet connection is stable\")\n",
        "        return None, None\n",
        "\n",
        "def get_model_size(cache_dir):\n",
        "    \"\"\"Get the total size of downloaded model files\"\"\"\n",
        "    total_size = 0\n",
        "    for path in Path(cache_dir).rglob('*'):\n",
        "        if path.is_file():\n",
        "            total_size += path.stat().st_size\n",
        "    return total_size / (1024 * 1024 * 1024)  # Convert to GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LYVSzH5Utmy"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: Replace this with your actual token or set as environment variable\n",
        "YOUR_TOKEN = \"paste your token here\"\n",
        "CACHE_DIR = \"./model_cache\"\n",
        "\n",
        "print(\"Starting model download process...\")\n",
        "print(\"Required disk space: approximately 14GB\\n\")\n",
        "\n",
        "tokenizer, model = setup_and_download_model(\n",
        "    token=YOUR_TOKEN,\n",
        "    cache_dir=CACHE_DIR\n",
        ")\n",
        "\n",
        "if tokenizer is not None and model is not None:\n",
        "    size_gb = get_model_size(CACHE_DIR)\n",
        "    print(f\"\\nTotal model size on disk: {size_gb:.2f}GB\")\n",
        "\n",
        "    print(\"\\nTesting with a simple input...\")\n",
        "    inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=50)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Test response: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "322AN1oqUtmz"
      },
      "source": [
        "## Interactive Lab Session\n",
        "\n",
        "In this lab, we'll build a simple cognitive agent that can remember and learn from conversations. We'll explore how to implement different types of memory and make our agent more context-aware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G030mG1XUtmz"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Dict\n",
        "import time\n",
        "\n",
        "# Verify installations\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_--tRi8Utm0"
      },
      "source": [
        "### 1. Understanding Memory Systems\n",
        "\n",
        "Just like humans, our agent needs different types of memory:\n",
        "- Short-term memory for recent conversations\n",
        "- Long-term memory for important information\n",
        "\n",
        "Let's implement a basic memory structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVP9zlb4Utm1"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Memory:\n",
        "    \"\"\"Simple memory structure for our cognitive agent\"\"\"\n",
        "    short_term: List[Dict[str, str]]  # Recent interactions\n",
        "    long_term: List[Dict[str, str]]   # Important information\n",
        "    max_short_term: int = 5           # Maximum number of recent interactions\n",
        "\n",
        "# Create a new memory instance\n",
        "memory = Memory(short_term=[], long_term=[])\n",
        "print(\"Memory system initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsWmnPqgUtm1"
      },
      "source": [
        "### Exercise 1: Adding Memories\n",
        "\n",
        "Let's try adding some memories to our system:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8M5_ideUtm2"
      },
      "outputs": [],
      "source": [
        "def add_memory(memory: Memory, text: str, is_important: bool = False):\n",
        "    \"\"\"Add a new memory to either short-term or long-term storage\n",
        "\n",
        "    Args:\n",
        "        memory (Memory): The memory system to update\n",
        "        text (str): The text to remember\n",
        "        is_important (bool): Whether this should go to long-term memory\n",
        "    \"\"\"\n",
        "\n",
        "    memory_entry = {\n",
        "        'content': text,\n",
        "        'timestamp': time.time()\n",
        "    }\n",
        "\n",
        "    # If the memory is important, add to long-term memory\n",
        "    if is_important:\n",
        "        memory.long_term.append(memory_entry)\n",
        "\n",
        "    # Always add to short-term memory\n",
        "    memory.short_term.append(memory_entry)\n",
        "\n",
        "    # If we've exceeded the short-term memory capacity,\n",
        "    # remove the oldest entry (first item in the list)\n",
        "    if len(memory.short_term) > memory.max_short_term:\n",
        "        memory.short_term.pop(0)\n",
        "\n",
        "# Test the implementation\n",
        "memory = Memory(short_term=[], long_term=[])\n",
        "\n",
        "# Test 1: Add a regular memory\n",
        "add_memory(memory, \"Hello, my name is Alice\")\n",
        "print(\"After adding first memory:\")\n",
        "print(f\"Short-term memories: {memory.short_term}\\n\")\n",
        "\n",
        "# Test 2: Add an important memory\n",
        "add_memory(memory, \"Remember my phone number is 555-0123\", is_important=True)\n",
        "print(\"After adding important memory:\")\n",
        "print(f\"Short-term memories: {memory.short_term}\")\n",
        "print(f\"Long-term memories: {memory.long_term}\\n\")\n",
        "\n",
        "# Test 3: Add several memories to test capacity\n",
        "for i in range(5):\n",
        "    add_memory(memory, f\"Test memory {i}\")\n",
        "print(\"After adding multiple memories:\")\n",
        "print(f\"Number of short-term memories: {len(memory.short_term)}\")\n",
        "print(f\"Short-term memory capacity: {memory.max_short_term}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsYw9La7Utm2"
      },
      "source": [
        "### 2. Implementing Memory Management\n",
        "\n",
        "Now let's create a system to decide what's important enough for long-term memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuemmC0jUtm3"
      },
      "outputs": [],
      "source": [
        "def is_important(text: str) -> bool:\n",
        "    \"\"\"Determine if a piece of information should be stored long-term\"\"\"\n",
        "    important_keywords = ['remember', 'important', 'key', 'crucial']\n",
        "    return any(keyword in text.lower() for keyword in important_keywords)\n",
        "\n",
        "# Test the importance detection\n",
        "test_texts = [\n",
        "    \"Remember to buy milk\",\n",
        "    \"The weather is nice\",\n",
        "    \"This is important information\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    print(f\"\\\"{text}\\\" is important: {is_important(text)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIBndctGUtm3"
      },
      "source": [
        "### Exercise 2: Enhanced Memory Management\n",
        "\n",
        "Can you improve the importance detection? Consider:\n",
        "- Additional keywords\n",
        "- Context analysis\n",
        "- User instructions\n",
        "- Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SfsLpMKUtm3"
      },
      "outputs": [],
      "source": [
        "def enhanced_importance(text: str) -> bool:\n",
        "    \"\"\"Your improved importance detection system.\n",
        "\n",
        "    Consider\n",
        "    1. Keywords and phrases\n",
        "    2. Punctuation emphasis\n",
        "    3. Personal information markers\n",
        "    4. Time-related information\n",
        "    5. Request patterns\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to analyze\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the text is considered important\n",
        "    \"\"\"\n",
        "    # Convert to lowercase for consistent matching\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # 1. Extended keyword list\n",
        "    important_keywords = [\n",
        "        'remember', 'important', 'key', 'crucial', 'essential',\n",
        "        'don\\'t forget', 'note', 'reminder', 'critical', 'urgent',\n",
        "        'must', 'need to', 'save', 'keep'\n",
        "    ]\n",
        "\n",
        "    # 2. Personal information markers\n",
        "    personal_info_patterns = [\n",
        "        'my name', 'phone', 'email', 'address', 'birthday',\n",
        "        'password', 'account', 'contact', 'social security',\n",
        "        'credit card', 'appointment', 'meeting'\n",
        "    ]\n",
        "\n",
        "    # 3. Time-related patterns\n",
        "    time_patterns = [\n",
        "        'tomorrow', 'next week', 'schedule', 'deadline',\n",
        "        'due date', 'appointment', 'meeting at'\n",
        "    ]\n",
        "\n",
        "    # Check for keyword matches\n",
        "    has_keyword = any(keyword in text_lower for keyword in important_keywords)\n",
        "\n",
        "    # Check for personal information\n",
        "    has_personal_info = any(pattern in text_lower for pattern in personal_info_patterns)\n",
        "\n",
        "    # Check for time-related information\n",
        "    has_time_info = any(pattern in text_lower for pattern in time_patterns)\n",
        "\n",
        "    # Check for emphasis through punctuation\n",
        "    has_emphasis = text.count('!') > 0 or text.count('?') > 1\n",
        "\n",
        "    # Immediate markers of importance\n",
        "    if has_keyword or has_personal_info:\n",
        "        return True\n",
        "\n",
        "    # Combined factors for importance\n",
        "    if (has_time_info and has_emphasis) or \\\n",
        "       (has_time_info and len(text.split()) > 10):  # Longer time-related messages\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# Test your implementation\n",
        "test_cases = [\n",
        "    # Keywords and explicit importance\n",
        "    \"Remember to send the report\",           # Should be True\n",
        "    \"This is important information\",         # Should be True\n",
        "    \"Just a casual message\",                 # Should be False\n",
        "\n",
        "    # Personal information\n",
        "    \"My phone number is 555-0123\",          # Should be True\n",
        "    \"My email is user@example.com\",         # Should be True\n",
        "    \"I like blue color\",                    # Should be False\n",
        "\n",
        "    # Time-related with emphasis\n",
        "    \"Meeting tomorrow at 3 PM!\",            # Should be True\n",
        "    \"Deadline for the project next week!\",   # Should be True\n",
        "    \"I'll be there tomorrow\",               # Should be False\n",
        "\n",
        "    # Complex cases\n",
        "    \"Don't forget about the important meeting with the client tomorrow morning!\",  # Should be True\n",
        "    \"Please make sure to keep my contact information saved for future reference\",  # Should be True\n",
        "    \"The weather is quite nice today\",       # Should be False\n",
        "]\n",
        "\n",
        "# Run tests\n",
        "print(\"Testing Enhanced Importance Detection:\\n\")\n",
        "for test in test_cases:\n",
        "    result = enhanced_importance(test)\n",
        "    print(f\"Text: \\\"{test}\\\"\")\n",
        "    print(f\"Important: {result}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK0_SWheUtm4"
      },
      "source": [
        "### 3. Building the Complete Cognitive Agent\n",
        "\n",
        "Now let's put everything together into a complete agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ea3h1j5Utm4"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict\n",
        "from threading import Lock\n",
        "\n",
        "class CognitiveAgent:\n",
        "    def __init__(self, model_path=\"./model_cache\", device_map=\"auto\"):\n",
        "        \"\"\"Initialize the cognitive agent with a local cached Phi-2 model\n",
        "\n",
        "        Args:\n",
        "            model_path (str): Path to the cached model files\n",
        "            device_map (str): Device mapping strategy for model loading\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_path,\n",
        "                torch_dtype=torch.float16,\n",
        "                device_map=device_map,\n",
        "                low_cpu_mem_usage=True\n",
        "            )\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\n",
        "                f\"Failed to load model from {model_path}. \"\n",
        "                \"Please ensure you've run the model download step first.\"\n",
        "            ) from e\n",
        "\n",
        "        self.memory = Memory(\n",
        "            short_term=[],  # Recent interactions\n",
        "            long_term=[],   # Important information\n",
        "            max_short_term=3  # Maximum number of recent interactions to keep\n",
        "        )\n",
        "        self.lock = Lock()\n",
        "\n",
        "        # Simplified system prompt for smaller context window\n",
        "        self.system_prompt = \"\"\"You are a helpful cognitive agent. Be clear and concise.\"\"\"\n",
        "\n",
        "    def _format_conversation(self) -> str:\n",
        "        \"\"\"Format the conversation history with system prompt\"\"\"\n",
        "        conversation = [f\"System: {self.system_prompt}\"]\n",
        "\n",
        "        # Add relevant long-term memories\n",
        "        if self.memory.long_term:\n",
        "            conversation.append(\"Important Context:\")\n",
        "            for memory in self.memory.long_term:\n",
        "                conversation.append(f\"- {memory['content']}\")\n",
        "\n",
        "        # Add recent conversation history\n",
        "        for interaction in self.memory.short_term:\n",
        "            conversation.append(f\"Human: {interaction['input']}\")\n",
        "            if 'response' in interaction:\n",
        "                conversation.append(f\"Assistant: {interaction['response']}\")\n",
        "\n",
        "        return \"\\n\".join(conversation)\n",
        "\n",
        "    def process_input(self, user_input: str, max_new_tokens: int = 512) -> str:\n",
        "        \"\"\"Process user input and generate a response\n",
        "\n",
        "        Args:\n",
        "            user_input (str): The user's input text\n",
        "            max_new_tokens (int): Maximum number of new tokens to generate\n",
        "\n",
        "        Returns:\n",
        "            str: The generated response\n",
        "        \"\"\"\n",
        "        # Format the conversation with context\n",
        "        conversation = self._format_conversation()\n",
        "        conversation += f\"\\nHuman: {user_input}\\nAssistant:\"\n",
        "\n",
        "        # Tokenize input\n",
        "        inputs = self.tokenizer(conversation, return_tensors=\"pt\", truncation=True)\n",
        "        inputs = inputs.to(self.model.device)\n",
        "\n",
        "        # Generate response\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                max_new_tokens=max_new_tokens,  # Use max_new_tokens instead of max_length\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode and clean up response\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        response = response.split(\"Assistant:\")[-1].strip()\n",
        "        response = response.split(\"Human:\")[0].strip()\n",
        "\n",
        "        # Update memory\n",
        "        self._update_memory(user_input, response)\n",
        "\n",
        "        return response\n",
        "\n",
        "    def _update_memory(self, user_input: str, response: str):\n",
        "        \"\"\"Update both short-term and long-term memory\"\"\"\n",
        "        with self.lock:\n",
        "            # Update short-term memory\n",
        "            self.memory.short_term.append({\n",
        "                'input': user_input,\n",
        "                'response': response,\n",
        "                'timestamp': torch.cuda.Event().record()\n",
        "            })\n",
        "\n",
        "            # Maintain short-term memory size\n",
        "            # Use the max_short_term from the Memory dataclass\n",
        "            if len(self.memory.short_term) > self.memory.max_short_term:\n",
        "                self.memory.short_term.pop(0)\n",
        "\n",
        "            # Update long-term memory if input seems important\n",
        "            if self._is_important(user_input):\n",
        "                self.memory.long_term.append({\n",
        "                    'content': f\"User mentioned: {user_input}\",\n",
        "                    'timestamp': torch.cuda.Event().record()\n",
        "                })\n",
        "\n",
        "    def _is_important(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Determine if information should be stored in long-term memory\n",
        "        \"\"\"\n",
        "        important_keywords = ['remember', 'important', 'key', 'crucial', 'essential']\n",
        "        return any(keyword in text.lower() for keyword in important_keywords)\n",
        "\n",
        "    def get_memory_status(self) -> Dict:\n",
        "        \"\"\"Return the current state of the agent's memory\"\"\"\n",
        "        return {\n",
        "            'short_term_count': len(self.memory.short_term),\n",
        "            'long_term_count': len(self.memory.long_term),\n",
        "            'recent_interactions': self.memory.short_term[-3:] if self.memory.short_term else []\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk47IzT9Utm5"
      },
      "source": [
        "### Exercise 3: Testing Your Agent\n",
        "\n",
        "Create a series of interactions to test your agent's memory capabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTce4e23Utm5"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import Optional\n",
        "\n",
        "def test_agent(agent: Optional[CognitiveAgent] = None,\n",
        "               max_new_tokens: int = 50,\n",
        "               test_inputs: Optional[list] = None) -> None:\n",
        "    \"\"\"\n",
        "    Test the cognitive agent with timing information and progress updates.\n",
        "\n",
        "    Args:\n",
        "        agent: Existing agent instance or None to create new one\n",
        "        max_new_tokens: Maximum number of tokens to generate per response\n",
        "        test_inputs: List of test inputs or None to use defaults\n",
        "    \"\"\"\n",
        "    print(\"Starting agent test...\")\n",
        "\n",
        "    # Create agent if not provided\n",
        "    if agent is None:\n",
        "        print(\"Initializing new agent...\")\n",
        "        start_time = time.time()\n",
        "        agent = CognitiveAgent(model_path=\"./model_cache\")\n",
        "        print(f\"Agent initialization took {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "    # Default test cases if none provided\n",
        "    if test_inputs is None:\n",
        "        test_inputs = [\n",
        "            \"Hello! How are you?\",  # Simple greeting\n",
        "            \"What's 2+2?\",  # Short response\n",
        "            \"Remember that my name is Alice.\",  # Memory test\n",
        "            \"What's my name?\",  # Memory recall test\n",
        "        ]\n",
        "\n",
        "    total_time = 0\n",
        "    print(\"\\nRunning tests...\")\n",
        "    print(\"Note: First inference will be slower due to CUDA warmup\\n\")\n",
        "\n",
        "    for i, user_input in enumerate(test_inputs, 1):\n",
        "        print(f\"\\nTest {i}/{len(test_inputs)}\")\n",
        "        print(f\"Input: {user_input}\")\n",
        "\n",
        "        # Time the response generation\n",
        "        start_time = time.time()\n",
        "        response = agent.process_input(user_input, max_new_tokens=max_new_tokens)\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        print(f\"Response: {response}\")\n",
        "        print(f\"Generation time: {elapsed:.2f} seconds\")\n",
        "\n",
        "        total_time += elapsed\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(f\"\\nTest Summary:\")\n",
        "    print(f\"Total time: {total_time:.2f} seconds\")\n",
        "    print(f\"Average time per response: {total_time/len(test_inputs):.2f} seconds\")\n",
        "\n",
        "    # Print memory status\n",
        "    print(\"\\nFinal Memory Status:\")\n",
        "    memory_status = agent.get_memory_status()\n",
        "    print(f\"Short-term memories: {memory_status['short_term_count']}\")\n",
        "    print(f\"Long-term memories: {memory_status['long_term_count']}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"\\nGPU Memory Usage:\")\n",
        "        print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "        print(f\"Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ypPutBWUtm6"
      },
      "outputs": [],
      "source": [
        "agent = CognitiveAgent(model_path=\"./model_cache\")\n",
        "test_agent(\n",
        "    agent=agent,\n",
        "    max_new_tokens=20\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "iat460",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}